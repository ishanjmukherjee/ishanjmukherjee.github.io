---
layout: post
title: "The competing infinities of longtermism"
permalink: /competing-infinities
date: 2024-07-14
---

_Comments on [Substack](https://decisiontree.substack.com/p/competing-infinities)._

<img src="/assets/images/lighthouse-unsplash.jpg" alt="A picture of a lighthouse from Unplash. To set the moooood." width="auto">

Either a present-day person is near-infinitely more important than a person from the distant future, or we have near-infinitely large obligations to future populations. There is no middle ground. Which infinity do we pick?

---
<br>
The central idea in this post won’t be groundbreaking to those familiar with [longtermism](https://en.wikipedia.org/wiki/Longtermism) — the view that the welfare of future beings is a key moral priority. Still, I am writing it because I haven’t heard the idea framed in quite this way.

Economists talk about a *social discount rate*, i.e., the rate at which you care less about social goods as time goes on. For example, if the social discount rate is 5%, you care 5% less about a bridge built next year than one built this year. The question is: what should the social discount rate be?

In some sense, we must at least *somewhat* discount future social goods. A year later, the world will be about 3% richer, so a bridge will matter less to it. (Thinking about the extreme case will be useful here: in a world with no bridges, building a new bridge is highly valuable; in a world with multiple bridges across every stream, an extra bridge is next to worthless.) We must also discount future goods because *a future might not exist*. We may be wiped out by a pandemic or a nuclear war before receiving the fruits of our delayed gratification.

Neither of these is the sense I’m interested in. **The question I’m interested in: is the *same unit of utility* (like the pleasure of eating an ice cream, or getting an extra year to live) worth less a year from now?** In other words, is a bridge built today worth even more than one and three hundredths of a bridge built next year? (Maybe think of the extra three hundredths as something that makes the bridge extra valuable, like [friezes on the bridgehouses](https://en.wikipedia.org/wiki/DuSable_Bridge#Decoration).) 

The fairly obvious answer is no. As Toby Ord writes in *The Precipice*, you can find academic philosophers willing to take either side of virtually any issue, but practically none ready to defend a social discount rate. It’s hard to argue that an 80-year life beginning in 1970 is intrinsically more valuable than one that started in 1980\. 

But let us run with a nonzero social discount rate for a moment. **Even at a modest rate of 1%, a present-day life would be worth ten thousand lives after just about 930 years. A million lives in under 1400 years. No matter how low your discount rate, saving a life in some arbitrarily distant future will be about as valuable as preventing a speck of dust from falling into someone’s eye today.** To put it mildly, this seems unacceptable. As Tyler Cowen and Derek Parfit [write \[PDF\]](https://d101vc9winf8ln.cloudfront.net/documents/27957/original/Cowen___Parfit_-_Against_the_social_discount_rate.pdf?1523454279), “Why should costs and benefits receive less weight, simply because they are further in the future? When the future comes, these benefits and costs will be no less real. Imagine finding out that you, having just reached your twenty-first birthday, must soon die of cancer because one evening Cleopatra wanted an extra helping of dessert. How could this be justified?” 

**Unfortunately, accepting a social discount rate of zero comes with its own problems. As *Our World in Data* [shows](https://ourworldindata.org/the-future-is-vast), even if the world population stabilizes at *just* 11 billion and humanity never leaves even *the Earth*, 125 quadrillion people are yet to be born.** That is 15 million for each present-day person. (If we think [digital people count morally](https://www.cold-takes.com/digital-people-faq/#could-digital-people-be-conscious-could-they-deserve-human-rights), this figure is even higher.) These future people will lead lives as happy and meaningful as ours — if not more, should the trend of [ever-increasing material prosperity](https://ourworldindata.org/grapher/global-gdp-over-the-long-run) continue or we [raise our hedonic set-point](https://www.hedweb.com/). **If all lives matter equally, the impact of our policies on present lives should be an afterthought at best.** All our obligations are to the yet to be.

What does this mean in practice? It means that most of the state’s resources should go towards [securing the existence of future people](https://nickbostrom.com/papers/astronomical-waste/). So, no progress in medicine or AI until the remotest threat of [misaligned superintelligence](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence) or [engineered superviruses](https://80000hours.org/problem-profiles/preventing-catastrophic-pandemics/) is gone. Actually, no, we need to go further. All technology should be positively *dismantled* — think of the small but nonzero risk of [runaway climate change causing extinction](https://en.wikipedia.org/wiki/Clathrate_gun_hypothesis).

**These, then, are the competing infinities. Either a present-day person is near-infinitely more important than a person from an arbitrarily distant future, or we have near-infinitely large obligations to future populations. There is no middle ground.**

Still, what if we try looking for one? Can we postulate that present lives matter more than future lives, yes, but *there’s a limit*? That no matter how far into the future you go, a life is worth a morally significant amount in present-day terms — say, taking a present-day man off dialysis, or restoring someone’s eyesight? 

I don’t think this will work. Unless you set this backstop to be trivially low, the sheer number of future lives will bring you back to the problem of an infinitely valuable future.

# Picking an infinity

It seems that we must pick an infinity. If so, the arguments for picking the infinity that favors present lives are rather bad.

First, you could argue that a social discount rate is fine because you have special obligations to people in your community, i.e., people alive today. 

I don’t think this argument works, because you have some obligations to people *not* in your community, too. If someone from your community and someone not are both drowning, saving your clansman is maybe defensible. But choosing him over a *million* people from your outgroup who are drowning? That seems clearly wrong.

Second, you could argue that our society-wide cost-benefit analyses should have a discount rate because people *in real life* seem to have a discount rate. Most people would rather have a marshmallow today than a marshmallow a year from now.

But this only shows that individuals seem to discount goods in time when choosing for themselves; it is unrelated to their views about delivering goods to *others*. (This seems to be borne out by a [survey](https://link.springer.com/article/10.1023/A:1022298223127); people report a “pure time preference” of zero for others, meaning no preference for current over future generations.) As Ord puts it, this suggests that “our preference for immediate gratification is really a case of weakness of will, rather than a sober judgment that our lives really go better when we have smaller benefits that happen earlier in time.” Moreover, even if people *do* have a discount rate, it doesn’t mean they *should* have a discount rate, much less that they should have a discount rate when choosing for *others*. A bright red line separates positive and normative economics.

Fortunately, I lied: there may be a way to avoid picking an infinity. If [Leopold Aschenbrenner](https://globalprioritiesinstitute.org/existential-risk-and-growth-aschenbrenner-and-trammell/) is right, we’re currently in an awkward time of perils: rich enough to have world-ending technology, too poor to do anything about it. More growth will bring more risk, for example by enabling investment in biotech that makes starting engineered pandemics [easier](https://x.com/patrickc/status/650726376073367552?lang=en). But if we halt technological progress now, we will be stuck in this bad place forever. On the other hand, growth (which is in the [best interests of present generations](https://www.goodreads.com/en/book/show/31283667-stubborn-attachments)) will give us wealth to [invest in existential risk reduction](https://reflectivedisequilibrium.blogspot.com/2020/05/what-would-civilization-immune-to.html) (which is in the best interests of future generations). It will enable us to slip out of this time of perils. Under this view, existential risk follows a U-shaped [Kuznets curve](https://en.wikipedia.org/wiki/Kuznets_curve) — a bump in existential risk followed by safety alongside prosperity. The interests of present and future people may really not be opposed at all.